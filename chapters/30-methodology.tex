\chapter{Methodology}\label{cha:method}
So far, relevant algorithms were covered in \autoref{cha:literature} and their implementation counterparts were discussed in \autoref{cha:software}. At that point, three capabilities were identified that made the decision tree induction implementation in scikit-learn inferior to that in Weka. For the highest priority issue (i.e., lack of pruning), the literature offered sufficient argumentation to immediately tackle this issue. The solution was presented in \autoref{cha:software_new}. At this point three tasks remain:
\begin{itemize}
    \item Validate whether the new pruning solutions work as expected
    \item Verify whether the accepted workaround for categorical attributes has a significant performance impact
    \item Find a more elegant solution to the missing values problem
\end{itemize}

At least for the first two tasks, experiments have to be performed. This chapter explains the experimental setup while \autoref{cha:results} covers the actual results.
%TODO chapter structure

% Datasets + description
% Architecture?

\section{Pruning implementation validation}
First, the experimental setup of the pruning validation is discussed. The new implementations of Reduced Error Pruning and Error Based Pruning are judged on three aspects: the size of the tree, the accuracy of the tree on unseen data and the running time of the algorithm. Multiple instances of the new \texttt{PruneableDecisionTreeClassifier} take part in this experiment, each with different settings. More specifically, the following configurations are used:

\begin{itemize}
    \item no pruning and no early stopping
    \item early stopping using $\texttt{min\_samples\_leaf} = \frac{0.5}{\texttt{n\_classes}}$ [scikit-learn exclusive]
    \item REP with $\texttt{rep\_val\_percentage} = 10\%$
    \item REP with $\texttt{rep\_val\_percentage} = 20\%$
    \item REP with $\texttt{rep\_val\_percentage} = 50\%$
    \item EBP with $\texttt{ebp\_confidence} = 10^{-5}$
    \item EBP with $\texttt{ebp\_confidence} = 10^{-3}$
    \item EBP with $\texttt{ebp\_confidence} = 10^{-1}$
\end{itemize}

Comparing only pruned versus unpruned trees does not tell the whole story. The pruned trees must also be compared against a sensible baseline of what scikit-learn users do if no pruning algorithm is available. That is why the early stopping configuration with \texttt{min\_samples\_leaf} is also in this list. Unfortunately, finding a good value of this parameter that works across a wide range of datasets is not easy. The current formula depends on the context to (partly) work around this issue: it takes the number of classes into account.

A comparison within scikit-learn of pruned versus unpruned trees with or without early stopping criteria is a good start, but an external benchmark against similarly configured J48 instances is also interesting. Although the base algorithm of the two software libraries differ, the goal is to maintain maximum compatibility. To that end, some additional rules must be taken into account. All other parameters are set to their default values.

\begin{itemize}
    \item All algorithms must produce binary trees
    \item Multilabel and multioutput capabilities must not be used
    \item Nodes must be split using the information gain criterion
    \item Early stopping criteria must not be used unless explicitly specified in the configuration
    \item Advanced J48 features such as tree collapsing, subtree raising or MDL corrections must be disabled
\end{itemize}

For both scikit-learn and Weka, a grid search algorithm loops over all the listed configurations, fits them to a training set and finally compares the prediction made for an independent test set against the correct result. Because we start from a complete data set, it has to be split in a training set and a test set. Tenfold cross-validation takes care of this. For statistical robustness, the whole procedure is repeated ten times. Each configuration will thus be tested hundred times. Relevant statistics of each test are gathered by the built-in tools of both libraries. To have reproducible results, the seeds of the random number generators take fixed values before the experiment starts.

This whole procedure assumes one dataset was given as input. To get a more complete picture of the implementation performance, the test is repeated for various datasets. These datasets are introduced below in \autoref{ssec:datasets}.

Both scikit-learn and Weka offer tools to facilitate such tests and gather the statistics afterwards. Every configuration has a fairly close counterpart in the other tool. The main differences that remain after taking the above setup into account, are the different base algorithms (C4.5 versus CART), the different runtimes (Java versus Python) and the fact that the dataset in scikit-learn has to be pre-processed by the \texttt{CsvImporter} while Weka can handle the datasets out of the box.

\subsection{Datasets}
\label{ssec:datasets}
Part of the experimental setup is the choice of datasets. We opt for a variety of real world classification datasets to get a good picture of the pruning implementation's performance. These datasets are primarily taken from \url{www.openml.org}~\cite{openml}. Table \ref{tbl:datasets} gives an overview of all datasets.

\begin{table}
\centering
\begin{tabular}[htp]{ l l r r r l r }
    Name & Description & C & F & N & M & CF \\ \hline
    \href{https://www.openml.org/d/37}{diabetes} & Pima Indians diabetes & 2 & 8 & 768 & No & 0 \\
    \href{https://www.openml.org/d/59}{ionosphere} & Johns Hopkins ionosphere & 2 & 34 & 351 & No & 0 \\
    \href{https://www.openml.org/d/61}{iris} & Fisher's iris & 3 & 4 & 150 & No & 0 \\
    \href{https://www.openml.org/d/187}{wine} & Wine recognition & 3 & 13 & 178 & No & 0 \\
    \href{https://www.openml.org/d/1510}{wdbc} & Breast cancer Wisconsin & 2 & 30 & 569 & No & 0 \\
    \href{https://www.openml.org/d/6}{letter} & Letter image recognition & 26 & 16 & 20 000 & No & 0 \\
    \href{https://www.openml.org/d/823}{houses} & House price high/low & 2 & 8 & 20 640 & No & 0 \\
    \href{https://www.openml.org/d/53}{heart} & Heart disease & 2 & 13 & 270 & No & 0 \\
    \href{https://www.openml.org/d/334}{monks} & Monks problems & 2 & 6 & 601 & No & 6 \\
    \href{https://www.openml.org/d/50}{tic-tac-toe} & Tic-tac-toe endgame & 2 & 9 & 958 & No & 9 \\
    \href{https://www.openml.org/d/31}{credit-g} & Credit risk & 2 & 20 & 1 000 & No & 13 \\
    \href{https://www.openml.org/d/10}{lymph} & Lymphography & 4 & 18 & 148 & No & 15 \\
    \href{https://www.openml.org/d/56}{vote} & 1984 US votes & 2 & 16 & 435 & Yes & 16 \\
    \href{https://www.openml.org/d/55}{hepatitis} & Hepatitis survival & 2 & 19 & 155 & Yes & 13 \\
    % \href{https://www.openml.org/d/172}{shuttle} & Shuttle landing control & 2 & 6 & 15 & Yes & 6 \\
    \href{http://www.cis.fordham.edu/wisdm/dataset.php}{activity} & Activity prediction & 6 & 45 & 5 424 & Yes & 0 \\
\end{tabular}
\caption{Overview of real world classification datasets. Column C indicates the number of classes, F the number of features (excluding the class feature), N the number of observations, M whether the datasets contains missing values and CF the number of categorical features (also excluding class).}%
\label{tbl:datasets}
\end{table}

The \emph{activity} dataset is the odd one out. It belongs to a study by Kwapisz et al.~\cite{problematic_dataset}. In this study, the activity of a test subject is predicted based on sensor data from cell phone accelerometers. The possible activities are walking, jogging, going upstairs, going downstairs, sitting and standing. Scikit-learn's regular \texttt{DecisionTreeClassifier} performed poorly on this set in the past, although Weka's J48 handled it well.

\section{Evaluate need for categorical attributes}


\section{Evaluation}


\section{Conclusion}
