\chapter{Literature review}\label{cha:literature}
The relevant literature for this thesis mostly consists of papers concerning decision tree induction. These go back many decades, but fortunately there are some review and survey papers that provide a convenient overview~\cite{murthy1998automatic, rokach2005top, kotsiantis2007supervised}. On top of the academic literature, the source code and accompanying documentation of scikit-learn and Weka has also been a rich source of information.

\section{Prerequisites}
The reader ought to be familiar with basic machine learning concepts such as supervised learning, classification, regression, bias-variance trade-offs, model validation and ensemble learning. Furthermore, elementary knowledge of decision tree induction is expected. The most important basic concepts will be discussed briefly. Topics that are particularly important for the next chapters will be elaborated on.

\section{Scope}
A wide variety of decision tree induction algorithms exists. Here, only the \emph{top down induction of decision trees (TDIDT)} family is considered. It is the most common approach and it is particularly relevant to the software tools under scrutiny.

Unsupervised and semi-supervised algorithms are out of scope, as is online learning. Furthermore, only classification trees are considered. With little effort, most TDIDT classification algorithms can be converted to regression algorithms. Yet, these are far less popular and better alternatives such as XGBoost~\cite{xgboost} exist.

Ensemble methods are also out of scope. Recent decision tree algorithms rarely work with a single tree, but rather with an ensemble of trees. Random forests~\cite{rf} is a very popular example of bootstrap aggregating or \emph{bagging}. Regardless, the scope of this thesis concerns the fundamentals of decision trees, and not their derivatives. Implementation improvements suggested in this thesis could still potentially benefit related ensemble methods.

The algorithms in scope are all offline learning methods invented before the big data era. This implies that computation is done locally and that all data has to fit in memory. As such, online learning methods or distributed algorithms are out of scope.

Finally, only univariate tests are in scope. The test performed in each internal node must only evaluate one attribute of the observation. For categorical attributes, this typically implies checking whether or not the input is equal to a fixed category. For numerical (and thus ordered) attributes, the input value is compared against a fixed threshold using the less than or equal and greater than operators. Consequently, the input space is partitioned recursively using axis-aligned hyperplanes. This scope limitation precludes well-known but seldom used extensions such as oblique trees.

%TODO no rule generation
%TODO no logic trees, model trees

\section{Terminology}
Throughout the relevant literature, there is a lack of ubiquitous vocabulary shared by all researchers. Decision trees are used in various scientific fields, each with its own jargon. Specifically, there is a big divide between researchers that approach the problem from a machine learning perspective compared to those who come from a statistics background. To avoid confusion, some basic terms are reviewed. A \emph{decision tree} consists of \emph{(internal) nodes} which are connected to other nodes via a one-to-many \emph{parent-child} relation on one hand, and \emph{leaves} which have no children on the other hand. The \emph{root node} is the only node without parent. In a \emph{binary tree}, all internal nodes have two children.

Induction algorithms typically receive a \emph{training set} as input data to construct a decision tree while a \emph{test set} is used afterwards for model validation. These sets are tables of data where each row represents an \emph{observation}. All observation are fully described by a common set of \emph{attributes}. Some attributes are \emph{categorical}, others may be \emph{numeric}.\footnote{The latter is sometimes also referred to as \emph{ordered} because this is the underlying property of numbers the algorithm will use at some point. Strictly speaking categories can also have an implicit order. In that case, a good practice is to make this explicit and to convert each category to a unique number beforehand.} In a supervised learning context, one or more \emph{class labels} are also associated with each observation. If the total number of distinct class values equals two, the task is called \emph{binary classification}. Otherwise it is called \emph{multiclass classification}. \emph{Multilabel classification} occurs when one observation can be tagged with a variable number of class values at once. \emph{Multioutput classification} on the other hand occurs when multiple distinct classes, each with their own set of values, have to be derived from the same set of attributes. This can be accomplished trivially by creating multiple trees, each handling one class. Regardless, combining them in one tree might offer performance benefits. In a way, multilabel classification is a special case of multioutput classification. Decision trees are one of the few machine learning algorithms that can handle all these modes of operation natively.

During \emph{training}, first one root node is created and all observations in the training set are stored in this node. When a node is \emph{split} using some \emph{test function}, this function partitions the observations in subsets and then creates a child node for each subset. This process is repeated recursively until some stopping criterion is reached. The \emph{purity} of a node is defined as the percentage of observations in that node that belong to the majority class. A \emph{pure node} is a node with 100\% purity.

\section{A generic TDIDT algorithm}
A typical TDIDT algorithm for classification consists of two phases: a grow phase and an optional prune phase. The grow phase requires three subroutines with fixed signatures: a test generation function, a splitting function and a stopping function. Historically, researchers presented their TDIDT algorithms with fixed subroutines. Because of the common interface it is now common to choose these functions \emph{\`{a} la carte}. One could try to evaluate the performance of each function separately, but choosing the best of each function does not guarantee a global optimum. Holistic tests must be performed to ensure the best configuration is chosen. Also note that the efficacy of each combination seems to depend on the domain in which it is applied~\cite{mingers1989empirical}.

\subsection{Univariate test generation}
Based on the observations in a node, tests can be devised that spit those observations in a number of subsets. The goal of this step is to generate a finite number of tests $\tau_i \in \mathcal{T}$ based on one given attribute. Recall the tests based on multiple attributes exist but are out of scope. In the next step, one specific test is chosen from this set of possible tests.

Generating tests for categorical attributes is trivial. For binary trees the value of the attribute is compared against one specific category. If it matches, it belongs to the first subset, else to the second. This results in as many tests as there are possible categories for the attribute. For non-binary trees, one test suffices that maps each distinct category to a specific subset.

In the case of numeric, ordered attributes, threshold are introduced to partition the observations based on that ordering. That way, an infinite number of tests can be generated, which is of course undesirable. However, at least for the training data, not all tests will result in a different partitioning. A clever choice of thresholds should bring the number of subsets back to a manageable level.
%attribute selection > splitting

%ignore: categorical attributes can have an implicit ordering

\subsection{Splitting}
Classic TDIDT algorithms work by recursively splitting nodes based on some optimal test $\tau \in \mathcal{T}$, the set of all possible tests. A heuristic called the splitting criterion is required to determine this $\tau$. A few such criteria have stood the test of time.

\subsubsection{Purity}
The perfect test $\tau^*$ creates a partition $\mathcal{S}_{\tau^*} = \{S_1, \ldots, S_k\}$ wherein each subset is pure, so optimizing for weighted average partition purity is a sensible first criterion.

\begin{equation}
    p(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} p(S_i)
\end{equation}

Here, $S = S_1 \cup \ldots \cup S_k$ and $p(S)$ is the set purity as described above.
%can also be expressed as impurity instead using the same pattern as below

\subsubsection{Entropy and information gain}
In practice purity does not appear to work very well. That is why researchers came up with an alternative based on Shannon's information theory~\cite{shannon1948mathematical}. Quinlan used such metrics in many of his prominent algorithms such as ID3 and C4.5~\cite{id3ter, c45}, but it was already invented earlier for the Concept Learning System (CLS)~\cite{cls}. Define entropy (or missing information) of a variable $V$ with possible values $v_i$ and associated probabilities $p_i$ as follows:

\begin{equation}
    s(V) = - \sum_i p_i \log_2(p_i)
\end{equation}

The same concept can be applied to the class variable. Define the class entropy $s_C(S)$:

\begin{equation}
    s_C(S) = - \sum_c p(c) \log_2(p(c))
\end{equation}

where $p(c)$ is the probability that a random observation in $S$ belongs to class c. This value can be defined for any node, independent of any specific partition.

For a given test $\tau$, a similar definition can be given for each subset $S_i$ of the induced partition on $S$:

\begin{equation}
    s_C(S_i) = - \sum_c p_i(c) \log_2(p_i(c))
\end{equation}

For the entropy of the whole partition $\mathcal{S}_\tau$, again use the weighted average entropy of its subsets:

\begin{equation}
    s_C(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} s_C(S_i)
\end{equation}

Finally, calculate the information gain $h_{IG}(\tau, S)$ of the split that resulted from test $\tau$:

\begin{equation}
    h_{IG}(\tau, S) = s_C(S) - s_C(\mathcal{S}_\tau)
\end{equation}

where $\mathcal{S}_\tau$ is the partition resulting from test $\tau$.

\subsubsection{Gain ratio}
The information gain criterion is biased towards tests with many possible outcomes. This could be a problem in non-binary trees. The gain ratio alleviates this problem. First define split information $SI(\tau, S)$ --- the maximum possible information gain --- as follows:

\begin{equation}
    SI(\tau, S) = - \sum_i \frac{|S_i|}{|S|} \log_2 \frac{|S_i|}{|S|}
\end{equation}

Finally, define the gain ratio:

\begin{equation}
    h_{GR}(\tau, S) = \frac{h_{IG}(\tau, S)}{SI(\tau, S)}
\end{equation}
%watch out for instability due to very low SI! -> only calculate when high enough

In binary trees, this heuristic typically causes a less balanced tree compared to the information gain criterion~\cite{c45}.

\subsubsection{Gini}
Distance metrics such as the Gini impurity index can be used instead of heuristics based on information theory~\cite{cart}. The definitions follow the same pattern as those of the information gain:

\begin{equation}
    g(S) = \sum_c p(c)(1 - p(c))
\end{equation}
\begin{equation}
    g(S_i) = \sum_c p_i(c)(1-p_i(c))
\end{equation}
\begin{equation}
    g(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} g(S_i)
\end{equation}
\begin{equation}
    h_G(\tau, S) = g(S) - g(\mathcal{S}_\tau)
\end{equation}

% \subsubsection{Comparison}

\subsection{Stopping}
Breiman argues in his CART book~\cite{cart} that choosing good stopping criteria is far more important than choosing good splitting criteria. If early stopping was not applied or no pruning (see below) was performed afterwards, trees would grow excessively large on real world data sets. This is a classic case of overfitting. It negatively impacts many factors that make decision trees attractive in the first place, such as their comprehensibility and their fast training and inference. It is also detrimental to the performance of the model on unseen data since the model fails to generalize properly.

There are simple and more complex stopping criteria. Simples ones are based on features such as these:
\begin{enumerate}
    \item tree depth
    \item number of leaves
    \item number of observations in a node
    \item purity of a node
\end{enumerate}

More complex stopping criteria are based on the Minimum Description Length (MDL) of a tree~\cite{mdlstopping} or on statistical techniques such as a $\chi^2$-test. Quinlan proposed to use the latter in his ID3 algorithm but decided not to include it in the successor (C4.5)~\cite{id3ter, c45}.

\subsection{Pruning}
A better alternative to early stopping criteria is to let the tree grow freely, and to prune it afterwards in a bottom-up fashion. Typically, the current error estimate of the subtree rooted at the given node is compared to what the estimated error would be if this node was converted to a leaf by pruning away its descendants. If it would perform better as a leaf, the descendants are effectively pruned away. Many different pruning algorithms exists. What follows is a non-exhaustive list of common pruning approaches.

\subsubsection{Reduced Error Pruning (REP)}
Reduced Error Pruning is one of the most straightforward and statistically sound methods of pruning a tree~\cite{quinlan1987simplifying, repanalysis, elomaa2001analysis}. Instead of using the whole training set to grow the tree, some randomly chosen observations are withheld in a separate validation set. By using this validation set after the growth phase is completed, an unbiased estimate of the error of each node in the tree can be calculated. Nodes at the bottom of the tree are converted into leaves if the estimated error of the leaf is equal to or less than the estimated error of the subtree rooted at the given node. This process is repeated recursively until the smallest possible tree is obtained with the minimum estimated error based on the validation set.

The disadvantage of this method is that less data is available for growing the tree, potentially negatively impacting this process. This is not a concern if training data is available in abundance.

\subsubsection{Error Based Pruning (EBP)}
Error Based Pruning is a technique used in C4.5~\cite{c45}. It does not require a separate validation set, so the full training set can be used to grow the tree. The downside of this is that this method is less statistically sound. An upper bound is calculated based on the training error and that upper bound is used instead of the original error in comparisons. Generally speaking: if a node is associated with fewer observations, then there is less certainty about the error and the upper bound will be further away from the original value. %TODO elaborate

\subsubsection{Cost Complexity Pruning}
Cost Complexity Pruning, used in the CART algorithm~\cite{cart}, takes another approach akin to regularization in classic optimization problems. First, it generates a series of pruned trees based on the original. Then it considers both the total training error and a cost factor proportional to the size of each tree to make a first selection. If the training error increases due to the pruning, but it is compensated for by a much smaller tree, the operation as a whole can still be considered positive depending on a trade-off factor. The final tree is chosen from this first selection using a separate validation set. As such, the same drawbacks apply here as for Reduced Error Pruning.

\subsubsection{Others}
Many other pruning algorithms exist~\cite{mingers1989empirical, breslow1997simplifying, elomaa1999biases, esposito1997comparative}. The reader can find some inspiration in the following list:

\begin{enumerate}
    \item Minimum error pruning~\cite{niblett1986learning}
    \item Pessimistic pruning~\cite{mansour1997pessimistic, quinlan1987simplifying, c45}
    \item MDL-based pruning~\cite{mdlpruning, quinlan1989inferring}
    \item Critical Value Pruning~\cite{mingers1987rule}
    \item Pruning using back propagation~\cite{backproppruning}
\end{enumerate}

\subsubsection{Alternative: Rule-based Pruning}
An outlier in this list is Rule-based Pruning. Decision trees can be converted to a series of if-then statements where the condition is a conjunctive clause. These statements can be further simplified to if-then-else statements and then optimized, which can be seen as an alternative form of pruning. The resulting model is no long a tree, but it can still approximate the underlying concept that the tree used to represent.

\section{Conclusion}
TDIDT algorithms incorporate different subroutines, for each of which a number of alternatives are available. This makes them a very flexible tool with uses in a variety of settings. Popular algorithms such as C4.5 and CART are opinionated in the sense that they each propose a small number of specific configurations of components. Fortunately, that does not stop algorithm implementers from offering more choice to their users, as shown in the next chapter. Note also that there is no single precise definition of ID3, C4.5 or CART. New insights were acquired over time and added to the solution, but the algorithm name rarely changed.