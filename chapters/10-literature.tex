\chapter{Literature review}\label{cha:literature}
The relevant literature for this thesis mostly consists of papers concerning decision tree induction. These go back many decades, but fortunately there are some review and survey papers that provide a convenient overview~\cite{murthy1998automatic, rokach2005top, kotsiantis2007supervised}. On top of the academic literature, the source code and accompanying documentation of scikit-learn and Weka have also been a rich source of information.

\section{Prerequisites}
The reader ought to be familiar with basic machine learning concepts such as supervised learning, classification, regression, overfitting, model validation and ensemble learning. Furthermore, elementary knowledge of decision tree induction is expected. The most important basic concepts will briefly be recapitulated. Topics that are particularly important for the next chapters will be elaborated on.

\section{Scope}
A wide variety of decision tree induction algorithms exists. Here, only the \emph{top down induction of decision trees (TDIDT)} family is considered. It is the most common approach and it is particularly relevant to the software tools under scrutiny.

Unsupervised and semi-supervised algorithms are out of scope. Furthermore, only classification trees are considered. With little effort, most TDIDT classification algorithms can be converted to regression algorithms. Yet, these are far less popular than their classification counterparts and better alternatives such as XGBoost~\cite{xgboost} exist.

Ensemble methods are also out of scope. Recent decision tree algorithms rarely work with a single (complex) tree, but rather with an ensemble of relatively simple trees. Random forests~\cite{rf} is a very popular example of bootstrap aggregating or \emph{bagging}. Regardless, the scope of this thesis concerns the fundamentals of decision trees, and not their derivatives. On the other hand, implementation improvements suggested in this thesis can potentially benefit related ensemble methods. Other derivatives that are out of scope include logic trees and model trees.

The algorithms in scope are all offline learning methods. This is also known as batch learning. It implies that all computation is done in one place and that all data has to fit in memory. Online learning has gained relevance in the big data era since it does not impose such restrictions. On the other hand, the technique is only used if the context requires it because the performance is not up to par with offline learning.

Finally, only univariate tests are in scope. The test performed in each internal node must only evaluate one attribute of the observation. For categorical attributes, this typically implies checking whether or not the input is equal to a fixed category. For numerical (and thus ordered) attributes, the input value is compared against a fixed threshold using the less than or equal operator. Consequently, the input space is partitioned recursively using axis-aligned hyperplanes. This scope limitation precludes well-known but unpopular extensions such as oblique trees.

\section{Terminology}
Throughout the relevant literature, there is a lack of ubiquitous vocabulary shared by all researchers. Decision trees are used in various scientific fields, each of which has its own jargon. Specifically, there is a big divide between researchers that approach the problem from a machine learning perspective compared to those who come from a statistics background. To avoid confusion, we avoid synonyms and always use the same term for the same concept. In the following paragraphs, some basic terms are reviewed. 

A \emph{decision tree} consists of \emph{(internal) nodes} which are connected to other nodes via a one-to-many \emph{parent-child} relation on one hand, and \emph{leaves} which have no children on the other hand. The \emph{root node} is the only node without parent. In a \emph{binary tree}, all internal nodes have two children.

Induction algorithms typically receive a \emph{training set} as input data to construct a decision tree while a \emph{test set} is used afterwards for model validation. These sets are tables of data where each row represents an \emph{observation}. All observation are fully described by a common set of \emph{attributes}. Some attributes are \emph{categorical}, others may be \emph{numerical}.\footnote{The latter is sometimes also referred to as \emph{ordered} because this is the underlying property of numbers the algorithm will exploit at some point. Strictly speaking categories can also have an implicit order. In that case, a good practice is to make this explicit by numerically encoding them beforehand.} In a supervised learning context, one or more \emph{target attributes} are present. For classification tasks, target attributes are considered categorical even if they contain numeric data. The distinct values of a target attribute are called the \emph{class labels} of that attribute. A classification task is called \emph{binary classification} if there is exactly one target attribute which has exactly two associated class labels and each observation is associated with exactly one of the two class labels. If multiple class labels can be associated with an observation, the task is called \emph{multilabel classification}. If instead there are more than two class labels, it is referred to as \emph{multiclass classification}. \emph{Multioutput classification} on the other hand occurs when multiple target attributes, each with their own distinct set of class labels, have to be derived from the same set of (non-target) attributes. This can be accomplished trivially by creating multiple trees, each handling one class. However, combining them in one tree might offer performance benefits~\cite{vens2008decision, multioutput-better}. Decision trees are one of the few machine learning algorithms that can handle all these modes of operation natively.

During \emph{fitting}, first one root node is created and all observations in the training set are stored in this node. When a node is \emph{split} using some \emph{test function}, this function partitions the observations in subsets and then creates a child node for each subset. This process is repeated recursively until some stopping criterion is reached. The \emph{purity} of a node is defined as the percentage of observations in that node that belong to the majority class. A \emph{pure node} is a node with 100\% purity. When the tree is complete, it can \emph{predict} the output based on input from, for example, the test set.

\section{A generic TDIDT algorithm}
A typical TDIDT algorithm for classification consists of two phases: a grow phase and an optional prune phase. The grow phase requires three subroutines with fixed signatures: a test generation subroutine, a node splitting subroutine and a stopping subroutine. Historically, researchers presented their TDIDT algorithms with fixed subroutines. Because of the shared interface it is now common to choose these subroutines \emph{\`{a} la carte}. One could try to evaluate the performance of each subroutine separately, but selecting the best subroutine for each slot separately does not guarantee a global optimum. Holistic tests must be performed to ensure the best configuration is chosen. Also note that the efficacy of each combination seems to depend on the domain in which it is applied~\cite{mingers1989empirical}.

\subsection{Univariate test generation}
Based on the set of observations in a node, tests can be devised that partition this set in a number of subsets. The goal of this step is to generate a finite number of tests $\tau_i \in \mathcal{T}$ based on one associated attribute. Recall the tests based on multiple attributes exist but are out of scope. In the next step, one specific test is chosen from this set of possible tests.

Generating tests for categorical attributes is trivial. For binary trees, each test compares its input (i.e., the value of the associated attribute of an observation) against one fixed value selected from all known values of the associated attribute. If the test is positive, the observation belongs to the first subset, else to the second. This results in as many tests as there are possible values for the associated attribute. For non-binary trees, one test suffices that maps each distinct attribute value to a specific subset. Figures~\ref{fig:categorical_binsplit} and~\ref{fig:categorical_split} illustrate the difference. Non-binary trees are typically shallower and thus faster to go through from top to bottom during the inference phase. On the other hand, it is an all-or-nothing approach. If an attributes contains 100 different categories where only a few of them are very relevant, is it worth it to add 100 child nodes to the current node? Binary trees have more flexibility in this regard, but they pay for it in depth.

\begin{figure}[htp]
\begin{center}
    \begin{tikzpicture}
        [
            boxed/.style         = {shape=rectangle, draw, align=center},
            sibling distance        = 10em,
            level distance          = 6em,
            edge from parent/.style = {draw, -latex},
            every node/.style       = {font=\footnotesize},
            ]
            \node [boxed] {Colour is Red?}
            child { node [boxed] {\ldots} edge from parent node [left] {True} }
            child { node [boxed] {Colour is Green?}
            child { node [boxed] {\ldots} edge from parent node [left] {True} }
            child { node [boxed] {\ldots} edge from parent node [right] {False} }
            edge from parent node [right] {False} 
            };
        \end{tikzpicture}
    \end{center}
    \caption{Example of nodes that perform binary tests associated with the attribute Colour. Two levels are needed to reach the same conclusion as in \autoref{fig:categorical_split}. Of course in practice not every value is equally relevant, so it is difficult to make statements about which approach is better without knowing the context.}%
    \label{fig:categorical_binsplit}
\end{figure}
    
\begin{figure}[htp]
\begin{center}
    \begin{tikzpicture}
        [
            boxed/.style         = {shape=rectangle, draw, align=center},
            sibling distance        = 10em,
            level distance          = 6em,
            edge from parent/.style = {draw, -latex},
            every node/.style       = {font=\footnotesize},
            % every node/.style = {shape=rectangle, draw, align=center}
            % sloped
        ]
        \node [boxed] {Colour}
            child { node [boxed] {\ldots} edge from parent node [left] {Is Red?} }
            child { node [boxed] {\ldots} edge from parent node [left] {Is Green?} }
            child { node [boxed] {\ldots} edge from parent node [right] {Is Blue?} };
    \end{tikzpicture}
\end{center}
\caption{Example of a node that performs a test associated with the attribute Colour. Since this target attributes contains three distinct class labels, the observations are partitioned over three child nodes.}%
\label{fig:categorical_split}
\end{figure}

In the case of numerical attributes, thresholds are introduced to partition the observations based on the implied ordering. That way, an infinite number of tests can be generated, which is of course undesirable. However, at least for the training set, not all tests will result in a different partitioning. A clever choice of thresholds should bring the number of subsets back to a manageable level. \autoref{fig:numerical_split} shows an example.

\begin{figure}[htp]%
\label{fig:numerical_split}
\begin{center}
    \begin{tikzpicture}
        [
            boxed/.style         = {shape=rectangle, draw, align=center},
            sibling distance        = 10em,
            level distance          = 6em,
            edge from parent/.style = {draw, -latex},
            every node/.style       = {font=\footnotesize},
        ]
        \node [boxed] {Height $\leqslant 180$cm?}
            child { node [boxed] {\ldots} edge from parent node [left] {True} }
            child { node [boxed] {\ldots} edge from parent node [right] {False} };
    \end{tikzpicture}
\end{center}
\caption{Example of a node that performs a test associated with the numeric attribute Height. In this case a threshold of 180cm was chosen, but of course any threshold is allowed as long as it divides the set of all height values in two non-empty subsets.}
\end{figure}

%attribute selection > splitting

\subsection{Splitting}
Classic TDIDT algorithms work by recursively splitting nodes based on some optimal test $\tau \in \mathcal{T}$, the set of all possible tests. A heuristic called the splitting criterion is required to determine this $\tau$. A few such criteria have stood the test of time.

\subsubsection{Purity}
Given the set of all associated observations of a node $S$, the perfect test $\tau^*$ induces a partition $\mathcal{S}_{\tau^*} = \{S_1, \ldots, S_k\}$ wherein each subset is pure, so optimizing for weighted average partition purity is a sensible first criterion.

\begin{equation}
    p(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} p(S_i)
\end{equation}

Here, $S = S_1 \cup \ldots \cup S_k$ and $p(S)$ is the set purity as described above.
%can also be expressed as impurity instead using the same pattern as below

\subsubsection{Entropy and information gain}
In practice purity does not appear to work very well. That is why researchers came up with an alternative based on Shannon's information theory~\cite{shannon1948mathematical}. Quinlan used such metrics in many of his prominent algorithms such as ID3 and C4.5~\cite{id3ter, c45}, but it was already invented earlier for the Concept Learning System (CLS)~\cite{cls}. Define entropy (or missing information) of a variable $V$ with possible values $v_i$ and associated probabilities $p(v_i)$ as follows:

\begin{equation}
    s(V) = - \sum_i p(v_i) \log_2(p(v_i))
\end{equation}

The same principle can also be applied to a set. When we do this for the set of observations $S$ and the classes $c_i$ associated with those observations, the class entropy $s_C(S)$ is defined as follows:

\begin{equation}
    s_C(S) = - \sum_c p(c) \log_2(p(c))
\end{equation}

where $p(c)$ is the probability that a random observation in $S$ belongs to class c. This value can be defined for any node, independent of any specific partition.

For a given test $\tau$, a similar definition can be given for each subset $S_i$ of the induced partition on $S$:

\begin{equation}
    s_C(S_i) = - \sum_c p_i(c) \log_2(p_i(c))
\end{equation}

For the entropy of the whole partition $\mathcal{S}_\tau$, again use the weighted average entropy of its subsets:

\begin{equation}
    s_C(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} s_C(S_i)
\end{equation}

Finally, calculate the information gain $h_{IG}(\tau, S)$ of the split that resulted from test $\tau$:

\begin{equation}
    h_{IG}(\tau, S) = s_C(S) - s_C(\mathcal{S}_\tau)
\end{equation}

where $\mathcal{S}_\tau$ is the partition resulting from test $\tau$.

\subsubsection{Gain ratio}
The information gain criterion is biased towards tests with many possible outcomes~\cite{harris2002information}. This could be a problem in non-binary trees. The gain ratio alleviates this problem. First define split information $SI(\tau, S)$ --- the maximum possible information gain --- as follows:

\begin{equation}
    SI(\tau, S) = - \sum_i \frac{|S_i|}{|S|} \log_2 \frac{|S_i|}{|S|}
\end{equation}

Finally, define the gain ratio:

\begin{equation}
    h_{GR}(\tau, S) = \frac{h_{IG}(\tau, S)}{SI(\tau, S)}
\end{equation}
%watch out for instability due to very low SI! -> only calculate when high enough

In binary trees, this heuristic typically causes a less balanced tree compared to the information gain criterion~\cite{c45}.

\subsubsection{Gini}
Distance metrics such as the Gini impurity index can be used instead of heuristics based on information theory~\cite{cart}. The definitions follow the same pattern as those of the information gain:

\begin{equation}
    g(S) = \sum_c p(c)(1 - p(c))
\end{equation}
\begin{equation}
    g(S_i) = \sum_c p_i(c)(1-p_i(c))
\end{equation}
\begin{equation}
    g(\mathcal{S}_\tau) = \sum_i \frac{|S_i|}{|S|} g(S_i)
\end{equation}
\begin{equation}
    h_G(\tau, S) = g(S) - g(\mathcal{S}_\tau)
\end{equation}

% \subsubsection{Comparison}

\subsection{Early stopping}
Breiman argues in his CART book~\cite{cart} that choosing good early stopping criteria is far more important than choosing good splitting criteria. If early stopping was not applied or no pruning (see below) was performed afterwards, trees would grow excessively large on real world data sets. This is a classic case of overfitting. It negatively impacts many factors that make decision trees attractive in the first place, such as their comprehensibility and their quick fitting and predicting. It is also detrimental to the performance of the model on unseen data since the model fails to generalize properly.

There are simple and more complex stopping criteria. Simples ones are based on statistics such as these:
\begin{enumerate}
    \item tree depth
    \item number of leaves
    \item number of observations in a node
    \item purity of a node
\end{enumerate}

More complex stopping criteria can be based on the Minimum Description Length (MDL) of a tree~\cite{mdlstopping} or on statistical techniques such as a $\chi^2$-test. Quinlan proposed to use the latter in his ID3 algorithm but decided not to include it in the successor C4.5~\cite{id3ter, c45} due to its unreliability.

\subsection{Pruning}
A better alternative to early stopping criteria is to let the tree grow freely, and to prune it afterwards in a bottom-up fashion. Typically, the current error estimate of the subtree rooted at the given node is compared to what the estimated error would be if this node was converted to a leaf by pruning away its descendants. If it would perform better as a leaf, the descendants are effectively pruned away. Many different pruning algorithms exists. What follows is a non-exhaustive list of common pruning approaches.

\subsubsection{Reduced Error Pruning}
Reduced Error Pruning (REP) is one of the most straightforward and statistically sound methods of pruning a tree~\cite{quinlan1987simplifying, repanalysis}. Instead of using the whole training set to grow the tree, some randomly chosen observations are withheld in a separate validation set. By using this validation set after the growth phase is completed, an unbiased estimate of the error of each node in the tree can be calculated. Nodes at the bottom of the tree are converted into leaves if the estimated error of the leaf is equal to or less than the estimated error of the subtree rooted at the given node. This process is repeated recursively until the smallest possible tree is obtained with the minimum estimated error based on the validation set.

The disadvantage of this method is that less data is available for growing the tree, potentially negatively impacting this process. This is not a concern if training data is available in abundance.

\subsubsection{Error Based Pruning}
Error Based Pruning (EBP) is a technique used in C4.5~\cite{c45}. It does not require a separate validation set, so the full training set can be used to grow the tree. The downside of this is that this method is less statistically sound because it is impossible to calculate an unbiased estimate of the error. Instead, an upper bound is calculated based on the training error and that upper bound is used instead of the original error in comparisons. Generally speaking, if a node is associated with fewer observations then there is less certainty about the error and the upper bound will be further away from the original value. The calculation also depends for a large part on the required level of confidence, which the user must provide.

\subsubsection{Cost Complexity Pruning}
Cost Complexity Pruning, used in the CART algorithm~\cite{cart}, takes another approach akin to regularization in classic optimization problems. First, it generates a series of pruned trees based on the original. Then it considers both the total training error and a cost factor proportional to the size of each tree to make a first selection. If the training error increases due to the pruning, but it is compensated for by a much smaller tree, the operation as a whole can still be considered positive depending on a trade-off factor. The final tree is chosen from this first selection using a separate validation set. As such, the same drawbacks apply here as for Reduced Error Pruning.

\subsubsection{Others}
Many other pruning algorithms exist~\cite{mingers1989empirical, breslow1997simplifying, elomaa1999biases, esposito1997comparative}. The reader can find some inspiration in the following list:

\begin{enumerate}
    \item Minimum error pruning~\cite{niblett1986learning}
    \item Pessimistic pruning~\cite{mansour1997pessimistic, quinlan1987simplifying, c45}
    \item MDL-based pruning~\cite{mdlpruning, quinlan1989inferring}
    \item Critical Value Pruning~\cite{mingers1987rule}
    \item Pruning using back propagation~\cite{backproppruning}
\end{enumerate}

\subsubsection{Alternative: Rule-based Pruning}
An outlier in this list is Rule-based Pruning. Decision trees can be converted to a series of if-then statements where the condition is a conjunctive clause. These statements can be further simplified to if-then-else statements and then optimized, which can be seen as an alternative form of pruning. The resulting model is no long a tree, so it is considered out of scope for this thesis, but it can still approximate the underlying concept that the tree used to represent.

\section{Conclusion}
TDIDT algorithms incorporate different subroutines, for each of which a number of alternatives are available. This makes them a very flexible tool with uses in a variety of settings. Popular algorithms such as C4.5 and CART are opinionated in the sense that they each propose a small number of specific configurations of components. Fortunately, that does not stop algorithm implementers from offering more choice to their users, as shown in the next chapter. Note also that there is no single precise definition of ID3, C4.5 or CART. New insights were acquired over time and added to the solution, but the algorithm name rarely changed.